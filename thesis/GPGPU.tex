\chapter{Understanding the GPGPU}\label{chp:GPGPU}

\chapterquote{Hardware: The parts of a computer system that can be
  kicked.}  {Jeff Pesis}

\quotebook{
  It is important to include the overhead of transferring data to and
  from the device in determining whether operations should be
  performed on the host or on the device.
}{CUDABPG}

% In order to create algorithms on the GPU that run faster than their
% CPU version, we need to understand the gpu.

\section{The Architecture of the GPU}

\subsection{Thread Model}

% Grid, blocks, warps and threads. We will only deal with the one
% dimensional case.

% It is a wide SIMD/SIMT, Single-Instruction / Multiple-Thread,
% machine. This means branching hurts. Alot!


\subsection{Memory Model}

\subsubsection{Global Memory}

% Slow, coalescene of data types with size 1, 2, 4, 8, and 16 bytes.

% Coalesced memory access, float4 instead of float3

% The alignment requirement can be forced: stuct __align__(8) {

% Mention textures and cache. The project developed for this thesis
% will not be using textures, since global memory also has cache as of
% 2.0 hardware.

\subsubsection{Registers and Local Memory}

Kernel variables that the compiler will most likely place in local memory are:

\begin{itemize}
  \item Any array that from the compilers point of view are dynamically indexed.
  \item Structures or arrays that are to large to fit inside registers.
  \item Any variable in the kernel, if the kernel has to many
    variables to place them all in register memory. This is referered
    to as \textit{register spilling}.
\end{itemize}

% Combat local memory by using launch bounds

\subsubsection{Shared Memory}

% Faster than global/local

% Used to overcome the limitations of global and local memory.

% Can be used as global cache on never CUDA architectures isntead of
% textures and shared mem. nice we laike

% Bank conflicts, nothing is ever as good as it seems.




% NVIDIA mentioned 3 optimization points

% Structures of arrays vs arrays of structs. Usefull when fx
% sorting. CUDPP article/forum. Plus cache performance.

% Unrolling loops, even when it means more work. Preprocess lower
% nodes yields nearly a 50% speedup from this.


\section{Wide SIMT primitives}

% Scan and compact

% cite sengupta paper

\quotebook{efficient solutions to parallel problems in which each
  output requires global knowledge of the inputs.}{Sengupta:2007}

% Useful for scattering data

% Give array example

\section{Optimization Techniques}

% Naive implementation

\section{Case Study: Reduction}

\subsection{Switching to shared memory}

% Does that make any difference on 2.0 archs?

\subsection{Using Registers}

% Storing the thread local result in both smem and register

\subsection{Loop unrolling}

% Unroll the loops.

% Then move the first up to where data is loaded into shared mem

% And move the last reduction down where the final result is written.




%\section{Case Study: Segmented Reduction}

% Alot of reductions and iterative algorithms. It is therefore
% important to know how to optimize these from a naive implementation.

% Algorithm from bla

%\subsection{Naive implementation}

%\subsection{Switching to shared memory}


%\subsection{Rewriting slow math operations}

% Pow is 'expensive' and can be replaced by a simple doubling of the
% indices.

%\subsection{Unrolling the loops}

% Can be done with a template parameter. Would still need to check
% range, but could remove the loop variables. Would need a few speciel
% cases for small kernelsizes (1 - 4 threads).

% Pros

% Shared memory is almost halved

% Only needs half the threads

% No dummy variables

% Cons

% Unrolling yields more instructions for executions that would only
% loop once. However since that only account for the last couple of
% nodes, it is more than made up for by all the previous
% nodes. Version 2 could be used for executions with 2 or less threads.

% Unrolling must also adds a check for out of bounds (or replaces the
% loop with that check) However making sure that single block bounding
% box reductions are never brought to the segmented reducer will
% ensure at least 2 values exist and one bounds check can be omitted.
