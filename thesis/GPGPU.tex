\chapter{Understanding the GPGPU}\label{chp:GPGPU}

\chapterquote{Hardware: The parts of a computer system that can be
  kicked.}{Jeff Pesis}



% Motivate the use of GPUs

Because of the evergrowing demand for new effects and more detail in
computer games and other 3D graphics applications, GPU's have seen a
massive increase in power over the last decade, as evidenced by
\reffig{fig:gflops}. A similar figure comparing the theoretical
throughput of GPU's and CPU's can be seen in \citebook{CUDAPG}. With
this in mind it is easy to understand why one would want to perform
ray tracing entirely on the GPU.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{GFLOPS}
  \caption{A comparison of the development of floating point
    operations pr. second on GPU's and CPU's. \\The figure is borrowed
    from Chapter 1 in \citebook{CUDAPG}}.
  \label{fig:gflops}
\end{figure}

% What is different form CPUs

Utilizing all this power effectively, however, is not straightforward
and in order to create algorithms on the GPU that run faster than their
CPU counterparts, we need to understand how the GPU works.

The reason behind the difference in theoretical throughput seen in
\reffig{fig:gflops}, is that the CPU is designed for handling all
kinds of problems, with a large cache at it's disposal and efficient
handling of control flow. The GPU on the other hand is designed for
high throughput of small, arithmetically intense, \textit{dataparallel
  programs}\footnote{Each processor performs the same instruction
  concurrently on multiple data elements.}, which is exactly what is
required of a graphics card processing thousands of independent
vertices and performing the same shading on hundreds of thousands of
color fragments.

Since the graphics card is designed to perform vertex and fragment
processing independently of their respective neighbouring threads,
this also means that when programming the graphics card, no
assumptions can be made about which thread is where in it's
execution. This presents a problem in cases where the $n$'th thread
depends on information from all previous $n-1$ threads, fx when
splitting data or calculating the minimum or maximum values of $N$
vertices. NVIDIA's CUDA remedies this somewhat by providing
synchronization commands, but since these will only synchronize a
subset of the running threads, the overall problem remains the same.

% Why not use GPU/CPU solutions

An obvious solution is ofcourse to perform easily parallisable
operations on the graphics card and leave the rest for the
CPU. However, the following quote comes to mind:

\quotebook{
  It is important to include the overhead of transferring data to and
  from the device in determining whether operations should be
  performed on the host or on the device.
}{CUDABPG}

So once it has decided to use the graphics card, one cannot simply
transfer data back to the CPU in order to perform some operation and
then transfer it back the GPU. The overhead would in many cases be too
large to see any performance increase at all.

%% Fortunatly Sengupta et al. has come up with a solution to the
%% scattering problem, which will be discussed further in
%% \refsection{sec:GPUprims}.


% Overview of the chapter

Finding effective GPGPU solutions to the above mentioned problems is
the motivation for this chapter, which is structured as follows.
First we shall take a look at the how threads and memory is organized
on the graphics card. Understanding this will be critical in
developing GPGPU efficient solutions. Then a section will present new
GPGPU scan primitives, which will help us to perform data scattering
and reductions on dataparallel hardware. I will then be discussing a
couple interesting optimization techniques on the GPGPU, while
applying them to a reduction case study.



\section{The Architecture of the GPGPU}

To understand the architecture of the GPGPU, we must first understand the
relationship and layout of threads and then the different kinds of
memory available to threads.

\subsection{Thread Model}

% Grid, blocks, warps and threads. We will only deal with the one
% dimensional case.

The graphics card is able to handle execution of more than a million
threads sequentially. Simply considering a graphics application
running at a 1440x900 resolution should convince anyone of this. While
above I have argued that all of these threads are executed
independently of each other, NVIDIA's CUDA architecture does place
these threads in a hierarchy, which provides programmers with some
control over thread execution.

At the lowest level threads are scheduled and executed completely
parallel in small groups called \textit{warps}\footnote{The term
  originates from weaving, the first parallel thread
  technology.\citebook{CUDAPG}}, usually with a warpsize of 32. While
threads in a warp have their own instruction counter and register
state, and therefore logically can branch independently of
neighbouring threads, a warp can only execute one specific instruction
at a time. The following example should help clarify this.

\begin{algorithmic}
  \IF{threadID < 16}
    \ASSIGN{$x$}{$threadID$}
  \ELSE
    \ASSIGN{$x$}{$32 - threadID$}
  \ENDIF
\end{algorithmic}

The first 16 threads in the warp will evaluate to true and thus
perform the assignment $x \leftarrow threadID$, while the next 16
threads will evaluate to false and execute the alternate statement $x
\leftarrow 32 - threadID$. Since the warp can only perform one
distinct instruction at a time, it will have to first execute $x
\leftarrow threadID$, leaving the last 16 threads idle. It next
executes the else branch, meaning the first 16 threads are left
idling. While this example shows how branching can hurt performance,
when all threads in a warp do not take the same execution path,
knowning that all threads in the warp always are synchronized can also
be very beneficial, as will be shown in
\refsection{sec:loopUnrolling}.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{ThreadLayout}
  \caption{A figure of CUDA's thread layout.\\ The figure is borrowed
    from Chapter 2 in \citebook{CUDAPG}}.
  \label{fig:gflops}
\end{figure}


% It is a wide SIMD/SIMT, Single-Instruction / Multiple-Thread,
% machine. This means branching hurts. Alot!


\subsection{Memory Model}

\subsubsection{Global Memory}

% Slow, coalescene of data types with size 1, 2, 4, 8, and 16 bytes.

% Coalesced memory access, float4 instead of float3

% The alignment requirement can be forced: stuct __align__(8) {

% Mention textures and cache. The project developed for this thesis
% will not be using textures, since global memory also has cache as of
% 2.0 hardware.

\subsubsection{Registers and Local Memory}

Kernel variables that the compiler will most likely place in local memory are:

\begin{itemize}
  \item Any array that from the compilers point of view are dynamically indexed.
  \item Structures or arrays that are to large to fit inside registers.
  \item Any variable in the kernel, if the kernel has to many
    variables to place them all in register memory. This is referered
    to as \textit{register spilling}.
\end{itemize}

% Combat local memory by using launch bounds

\subsubsection{Shared Memory}

% Faster than global/local

% Used to overcome the limitations of global and local memory.

% Can be used as global cache on never CUDA architectures isntead of
% textures and shared mem. nice we laike

% Bank conflicts, nothing is ever as good as it seems.




% NVIDIA mentioned 3 optimization points

% Structures of arrays vs arrays of structs. Usefull when fx
% sorting. CUDPP article/forum. Plus cache performance.

% Unrolling loops, even when it means more work. Preprocess lower
% nodes yields nearly a 50% speedup from this.


\section{Wide SIMT primitives}\label{sec:GPUprims}

% Scan and compact

% cite sengupta paper

\quotebook{...efficient solutions to parallel problems in which each
  output requires global knowledge of the inputs.}{Sengupta:2007}

% Useful for scattering data

% Give array example

\section{Optimization Techniques}

% Naive implementation

\section{Case Study: Reduction}

\subsection{Switching to shared memory}

% Does that make any difference on 2.0 archs?

\subsection{Using Registers}

% Storing the thread local result in both smem and register

\subsection{Loop unrolling}\label{sec:loopUnrolling}

% Unroll the loops.

% Then move the first up to where data is loaded into shared mem

% And move the last reduction down where the final result is written.




%\section{Case Study: Segmented Reduction}

% Alot of reductions and iterative algorithms. It is therefore
% important to know how to optimize these from a naive implementation.

% Algorithm from bla

%\subsection{Naive implementation}

%\subsection{Switching to shared memory}


%\subsection{Rewriting slow math operations}

% Pow is 'expensive' and can be replaced by a simple doubling of the
% indices.

%\subsection{Unrolling the loops}

% Can be done with a template parameter. Would still need to check
% range, but could remove the loop variables. Would need a few speciel
% cases for small kernelsizes (1 - 4 threads).

% Pros

% Shared memory is almost halved

% Only needs half the threads

% No dummy variables

% Cons

% Unrolling yields more instructions for executions that would only
% loop once. However since that only account for the last couple of
% nodes, it is more than made up for by all the previous
% nodes. Version 2 could be used for executions with 2 or less threads.

% Unrolling must also adds a check for out of bounds (or replaces the
% loop with that check) However making sure that single block bounding
% box reductions are never brought to the segmented reducer will
% ensure at least 2 values exist and one bounds check can be omitted.
